% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.2 for LaTeX2e
%
\documentclass{llncs_Ibergrid2013}
%
\usepackage{makeidx}  % allows for indexgeneration
%
%\usepackage[dvips]{graphicx}
%
\usepackage{epsfig}
\usepackage{cite}
\usepackage{url}
\usepackage{microtype}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{eurosym}
\usepackage{url}
\usepackage{multirow}
\usepackage{listings}
\usepackage{natbib}
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
\addtocmark{Federated Appliance Distribution in a Federated Cloud} % additional mark in the TOC
%
\mainmatter              % start of the contributions
%
\title{Federated Appliance Distribution in a Federated Cloud}
%
\titlerunning{VM Image Management}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{A. Sim\'on\inst{1}, E. Freire\inst{1}, R. Rosende\inst{1}, I. D\'iaz\inst{1}, A. Feij\'oo\inst{1}, P. Rey\inst{1}, J. L\'opez-Cacheiro\inst{1}, C. Fern\'andez\inst{1} and O. Synge\inst{2}}
%
%\authorrunning{First Author et al.}   % abbreviated author list (for running head)
%
%%%% modified list of authors for the TOC (add the affiliations)
\tocauthor{First Author (Institution of first author's affiliation),
Second Author (Institution of second author's affiliation)}
%
\institute{Fundaci\'on Centro de Supercomputaci\'on de Galicia, Santiago de Compostela, Spain\\
\email{grid-admin@cesga.es}
\and
\email{owen.synge@jaysnest.de}
}




\maketitle              % typeset the title of the contribution

\begin{abstract}
With the development of Federated cloud infrastructures, Maintainers of virtual appliances need a mechanism to distribute thier work to sites, ideally without external dependancies that may delay critical updates to virtual appliances. This paper summarises the work developed within EGI FedCloud taskforce during the last year to deploy a sustainable Federated Appliance lifecycle Management System. This allows Appliance Maintainers to manage the lifecycle of Appliances on the Federated services they use without the need for centralised resources or services.
\end{abstract}

%
\section{Introduction}
\label{sect-introduction}
%
With virtual appliances on Cloud IaaS resources the complications for integrating multiple components and dependencies of a service can be managed closer to the appliance creator or maintainer without need for the cloud resource provider to be involved. Cloud IaaS has shown great commercial success, so much so that new cloud software stacks are being developed, and many companies and research institutes are deploying these clouds for both public and private use. EGI-inspire brings together many sites, running multiple IaaS Cloud implementations as a Federated cloud. As part of this effort they are making efforts to help abstract away these differences so that Virtual Appliance users and maintainers can remain IaaS implementation and site neutral.

The liberation of scientific computation from IaaS implentation lock in may have many economic and sustainability benifits as side effects for all IaaS computing.

Interoperability of Appliance distribution is one of the first dependencies in the process, of creating a federated cloud. This paper is focused on some of the issues surrounding Virtual Appliance Image Management in a federated cloud environment following the HVWG's proposals.


It is organised in the following sections. First, Section~\ref{sect-fedimagemanagement} presents the Aplaince distibution in the context of federated IaaS clouds, Section~\ref{sect-vmcaster} describes the VMcaster/VMcatcher image management tools, how these tools are federated, how it was implemented by EGI FedCloud project and how it was configured at CESGA. 

Section~\ref{sect-handlers} explains how VMcatcher event handlers work, and how allow different plugins for cloud frameworks like OpenNebula or OpenStack to synchronise VM images between heterogeneous cloud technologies. 
Finally Section~\ref{sect-conclusions} will present the conclusions and future work.  

%\subsection{User Services}

\section{Motivation}
\label{sect-motivation}
The priamary goal of a EGI-federatred cloud task force is to invetigate the potential of IaaS Clouds as a revolutionary upgrade to Grid Computing's useability. IaaS Clouds have been been deployued at many sites where they are actively used by communties that never succeeded with Grid Computation~\cite[?]. The EGI-federatred cloud task force are a testbed evaluating policies and technology for production use.

Secondary goals include providing a unification of the IaaS resources, from a users experiance and form resource sharing making for a quality of service to be extended beyond the capoabilities of a financially constrained resorce provider, Providing billing can be successfully implemented at least as well as Grid computing with apropriate poplicies. For the economic models and the details of billing are outside the scope of this paper with the exception that they are based upon signed messages for comunication.

This is very similar to the goals of Grid computing but making use of IaaS platforms. IaaS platforms have succeeded in the Market, while Grid computing succeeded only for big science with very homogenious workloads. It is the beleif of the author that this is related to IaaS Clouds, having far greater flexability for the Apliance Maintainer then the Grid Equivilent. The relatively new term DevOps is now common, in IT, and this person is often closly acociated with the role of Appliance management.
%\begin{figure}[h!]
%\centering
%\includegraphics[width=60mm,angle=90]{./PSFIGs/user-services.eps}
%\caption{Use case in IBERGRID}\label{figUSER}
%\end{figure}
\section{Related Work}
\label{sect-relatedwork}
The use of Cloud infrastructures in science has been documented, and it is a very promising field, but integrating Clouds with the Grid is a challenge, as related on Dillon et al.~\cite{Dillon2010}. Goasguen et al.~\cite{Goasguen2012} presents the results of an internal production cloud service in CERN and suggestions to expand it to another Grid sites. Zhao et al.~\cite{Zhao2012} presents a infrastructure of a dozen computing sites using OpenNebula as the management solution. It concludes that Clouds are very useful for science, but there are still many performance issues to be resolved. Hoffa et al.~\cite{Hoffa2008} reached similar conclusions regarding cloud vs local deployments.

There are also many works that compare the different solutions for VM Management, like Xiaolong et al.~\cite{Xiaolong2012}, which compares OpenNebula and Openstack, and Laszewski et al.~\cite{Laszewski2012}, which does a more complete survey including Eucalyptus, Nimbus and some other solutions. The existence of numerous trade-offs and fragmented market for these tools motivated us to support cross-grid environments.

On the realm of security, our solution emphasizes the authentication of users and the validation of VMs. There are other works on the area, but some, like Xi et al.~\cite{Xi2012} are concerned more with running trusted VMs on on untrusted environments, which can be seen as the opposite problem, and many others, like Schwarzkopf et al.~\cite{Schwarzkopf2012} are concerned with improving the internal security of VMs maintained by Cloud users instead of infrastructure operators.

There are still other comparable solutions, Lagar-Cavilla et al.~\cite{Lagar-Cavilla2009} use a non-local fork mechanism to spawn many copies of a VM across many sites, but this method would be at odds with current Grid practices. Diaz et al.~\cite{Diaz2012} have a similar system that bridges OpenNebula and OpenStack, but it uses the Amazon EC2 API, which has licensing issues preventing us for using it, and does not address the authorization and validation of VMs. On a more partial resemblance, Maurer et al.~\cite{Maurer2013} also automates some aspects of VM management and updates using an autonomous system, and Django et al.~\cite{Django2013} changes the context of VMs on the fly to do load balancing and improve brokering. This last functionality would be invaluable for Grid operators, which must frequently tend to processes that get stuck due to unrealistic brokering requirements, and would also avoid many downtimes due to reconfiguration.



\section{Federated Appliance Management}
\label{sect-fedimagemanagement}

This paper focuses on appliance distribution, in a federated cloud following the HVWG's recoemendations, it tries to detail where the available implementation of the subscriber falls short of the HVWG's recoemendations. It is the combined result of the EGI-federatred cloud task force collabourating with a maintainer of a HVWG image list subscriber, and a former memeber of the Hepix Virtualisation Working Groups, it focuses on the use of Appliance mangement through signd "image list" messages, in a similar way to package management in Linux, or simplified without security in a similar way to podcasts.

\section{Itentity management}
\label{sect-fedimagemanagement}
The HVWG proposal uses upon SMIME and so is esentially neutral of the method of signing and is applicable to pgp or x.509 signatures. This is not true for the vmcatcher implementation, which can only support the model provided with x.509 security. Since the Grid comunity already use the International Grid Trust Federation~\cite{igtf} just suporting x.509 is acceptable for the current stage for the Federated cloud task force. It is recomended that future HVWG image list providers support pgp and x.509 signatures.

\section{Appliance Lifecycle Management using Imagelists.}
\label{sect-appliancelifecycle}

are to aid users in isolating thier resources to a single IaaS

Since thier is no central authority wishes to control all deployments at all sites a the EGI federated cloud is 

The HVWG proposed a message system to decouple appliance maintainers from cloud implementations, this decoupling could be used to hide the hetrogenious nature of a federated cloud.

VMcaster~\cite{vmcaster} and VMcatcher~\cite{vmcatcher} generate and subscribe to virtual machine image lists~\cite{hepix} respecitivly. In many senses VMcatcher is similar to Debian's \textit{aptitude} or the rpm update management \textit{yum} utilities or a podcast subscriber, while VMcaster is a tool for making secure respotories in a similar way to \textit{create-repo}. This distributed model, requires appliance maintainers to provide a http server, and use a x509 certificate in making thier imagelists available, and finally inform potential subscribers of thier imagelists location. It is truely distributed and relies on no centralised server, so with suitable policy development can become a 

These tools try to match the requirements set by the now completed HEPiX virtualisation working group~\cite{hepix}.
The main task of this working group was to provide to sites a way to control and mange Virtual Machine (VM) Image's provided by experiments, and execute the VMs in a trusted environment (similar to the current computing environment provided under Grid computing). During the last year the EGI federated cloud task force has recommended VMcatcher as well for installation at all cloud resource providers in their collaboration and the status of the integration with OpenStack and OpenNebula.

In this case since the software is made with the Grid in mind VMcaster/VMcatcher tools are based on the International Grid Trust Federation~\cite{igtf}, which makes use of x.509 certificate authentication model. All the image lists are signed by an authenticated endorser with a personal x.509 certificate. All images are securly referanced referenced in a Virtual Machine Image List using a secure hash (SHA512) signed using X.509 personal certificates (provided by the image list endorser). These Virtual Machine Image Lists are published, and interested sites subscribe to the Lists in a catalogue at the site. The Federations of trust built earlier in Grid computing allowing multiple identiy proviers to be trusted, and the Hepix Virtualisation working group provides a template of for the security policies to be further developed by the Federated Cloud Task Force.

Imagelists are polled by the subscribers, thier signature checked, image downloaded and the validity as an appliance checked using the secure hash. If the applicance image is valid the applicance is then able to be contextualised and then instantiated (see figure \ref{fig:infrastructure}). 
Using this mechanism a virtual image can be checked for validity, all image lists are signed and it provide a version number and expiration date. If the image list does not satisfy these requirements the image is not instantiated and the request is rejected.

Another important feature of VMcaster/VMcatcher is the support of cloud framework agnostic tools, these tools do not depend on the cloud solution used by the sites. Besides it can be integrated with different frameworks using different plugins to use the new images directly from for example, OpenNebula or OpenStack (see section \ref{sect-handlers} for more information).

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{vmcaster_vmcatcher.png}
\caption{VMcaster and VMcatcher infrastructure.}
\label{fig:infrastructure}
\end{figure}

\subsection{Publishing image lists}
Publishing images according to the HVWG's proposals is as simple as generating the meta data and signing it with standards SMIME signature routines. No new software is needed for this task. Due to the amount of meta data recomeneded by the HVWG and the expectation that users may want to add thier own additional metadata, it is expected that users will use tools for generating the nessisarry JSON 

 HVWG image lists provide a simple interface for publishers to share thier images with many sites. Being a simple JSON metadata file signed with SMIME and referancing images with secure hases seems an apropriate interface for the Appliance maintainer.
 
VMcaster is the most automated tool managing and updating virtual machines image lists tested. Other implementations include a less that 50 line example implementation, and a template based imagelist generator.

lists which follows the HEPix image list specifications. It largely automates the process of managing images lifecycle through image lists. It is one of three image list publishing tools, that have been developed using the HVWG's suggested image list format.

VMcaster detects the image size and generates a SHA512 hash for each uploaded image, when this process is complete the updated information is included into the VMcaster database automatically. This can then be syncronised with the published signed image.


Reading the image list example in JSON format a future subscriber can indentify the available images and also the image list creation/expiration dates or the endorser DN.

This procedure allows different image endorsers to distribute and update image catalogs among different endpoints and sites (which is suitable for a federated architecture).
Besides all image lists has endorsed information about endorser certificate public key, image download endpoint, initial global validity, etc. 
That means valid images can be selected, downloaded and instantiated and tested by different resource provides. 
This task is done by VMcatcher utility described in the next section.

\subsection{VMcatcher}

It also suggests an XML and JSON support. Sadly the only available implementation only support JSON metadata and x509 signatures. Since this is the the meta data format matters little and the x.509 required secuurity infrastrcuture is available these limitations did not not hindered this investigation.

Currently the only subscriber to Image lists is called vmcatcher. The job of vmcatcher is to allow subscription of virtual maschien images by thier UUID so that a resorce provider can provide cintecxtulised apliance images from the most uptodate appliance Image. resource providers can configure their own imagelists and also select images from an image list, these are then validated in complainace with the Hepix security policies and cached locally. vmcatcher does not support any cloud exploistly so furterh work was needed in this area.

A federated cloud increases this difficulty still further, as different clouds with corresponding APIs, Appliance formats, and different contextualisation mechanism must be supported in a truly federated Cloud. Since the Hepix proposal only suggests managing Aplicance Lifecyle management with a single signed message, not all users will not need to manage Appliance updates in thier federations, 

VMcaster can be configured to launch appliacations on image update events for further applications to process, update or expire changes of Apliance images.  These events can be used by event listeners or event handlers to react to specific changes (when an image is set as invalid or a it is available a new image update). 

Fedcloud resource providers are using a heterogenous cloud frameworks ecosystem (OpenStack, OpenNebula, WNoDES~\cite{wnodes}, etc). This hetrogenious federated infrastructure can casue users to be presented with a complex aray of interfaces. 


\section{Image management event handlers}
\label{sect-handlers}
VMcatcher and VMcaster are useful tools to disseminate and keep updated our images but they do not interact with cloud frameworks directly.
VMcatcher was written in Python and generates pre-defined events that can be received by an asynchronous callback subroutine or event handler.
Fortunately the cloud community has developed event handlers to interact with the most popular frameworks like OpenNebula or CloudStack.
OpenNebula event handler~\cite{onevent} was developed by the CESGA team and currently is available from the VMcaster repository. 
The \textit{vmcatcher\_eventHndlExpl\_ON} package provides a new python and cron script which detects VMcatcher events. 
This script detects several VMcatcher event types. In this case OpenNebula handler only waits for a new Expire or Available VMcatcher event.
If VMcatcher raises a \textit{AvailablePostfix} event, this is detected by \textit{vmcatcher\_eventHndlExpl\_ON} event handler and reads the new image attributes such UUID, image name, description and image format.
\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{ONeventhandler.png}
\caption{OpenNebula event handler.}
\label{fig:onevent}
\end{figure}
If a new image is downloaded (\textit{AvailablePostfix} event), the OpenNebula event handler gets the image information, generates a new OpenNebula template and includes the new image into the local OpenNebula datastore (see figure \ref{fig:onevent}). 
For security reasons, the new images are not public, they are only available for the oneadmin user. The OpenNebula administrator should verify the new image first (checking its contextualisation script, if the image is executed correctly etc).
After this period of time the image status is changed to be available for external users. 
Moreover if VMcatcher detects an image revocation the OpenNebula event handler search the image UUID from OpenNebula image database and it is set to disable status.
The image is not removed by the event handler, it should be removed by the site administrator from the OpenNebula datastore.

The OpenNebula event handler is not the only one. OpenStack administrators can also use Glancepush~\cite{glancepush} service to keep their local image catalog updated. 
This service was developed at IN2P3 and it works in a similar way than the OpenNebula event handler. 
In this case Glancepush updates the OpenStack Image Service (Glance) if it detects any image change from VMcatcher tool. 
The new package (\textit{glancepush-vmcatcher}) is available from IN2P3 ftp server, and it only requires a working glance service and an OpenStack user account to push images into the catalog.

%\section{Use Case: EGI SA2.3 verification image repository}
%\label{sect-usecase}
%This is my text 


\section{Experiance from testing the HVWG proposals}
\label{sect-experiances}
As demonstrated in the Federated Cloud Demo (we need a citation), vmcatcher and vmcaster care available and relatively east to deploy. Investigation shows they operate without a single point of failure. Once CESGA impleemnted the nessisary handler to integrate vmcatcher with Open Nebular and similar work was provided by IN2P3 for open stax images can be published for both platforms as contextuliastion validated. Fully automated image updates between more than one site in both directions have been shown to work. Combined with the Hepix security policy, the apliance management system in testing has shown itself to be a basis of a full automated, federated image distribution system in practice and currently prodcution grade.

Testing and practice has shown that not all appliance producers regularly upgrade thier images. If applince maintianers do update it is unusual that they do so at the frequancy expected by the Hepix virtualisation working group. By providing greater isolation at the netwrok level, NIKEF and Oxford University in particular, have long suggested that they are capable of running insecure images, without fear of applinaces negatively effecting each other. CESGA is planing on developing policies and practices to also atchive this in the future. 

In light of the flexability allowed by greater nextork isolation, the monthly update recomendations from the Hepix Virtualisation Working group now seem overly optomistic. The flexabilities in Cloud netowrking capabilities where not forseen in the Hepix proposals and consiquently the ability to run poterntially outdated Appliances in a secured networking enviroment, where not addressed by the HVWG. This said we still find value in the secuirty model of the HPVWG's proposals, especially the use of IGTF based identity provison, the clear defintion of valid and invalid images, and the Hepix image endorsement policies as a guide for future work in the Federated cloud task force.

\section{Continued use beyond the prototype}
\label{sect-experiances}


A new use cases is the EGI SA2.3 verification image repository. The EGI SA2 testbed is used to verify and test the new middleware before reaching the production software repository (UMD).
One of the most important new features is the ability to distribute and publish VM images in an automated way. 
These new tools, like EGI MarketPlace\footnote{EGI MarketPlace: \url{http://marketplace.egi.eu}} and VMcatcher\footnote{VMcatcher: \url{https://github.com/hepix-virtualisation/vmcatcher}} can be also used within SA2 verification process to distribute and publish new UMD services after its verification. 
This work is still on going and it will be available in the next months. Using this infrastructure the new EGI certified image it will be available to be used and tested by EGI site administrators after each successful verification.

For the moment after each software verification the images are stored locally into CESGA OpenNebula datastore but thanks to image management tools like VMcaster the new images will be published in an automated way.
This new paradigm will be useful to users that want to check the latest software changes without the need to install a new service from scratch.
Fortunately as we have explained in this paper, VMcatcher can be used by any cloud framework to distribute and update images in a transparent way. 
The new image management tools are being used by EGI Fedcloud providers since last year successfully and probably it will be used in more use cases in the near future.


\section{Results}
\label{sect-results}

For these reasons we consider that the infrastructure provided by the Hepix Virtualisation working group forfills the needs of a EGI Tederated Cloud tasjkforce but recomend that the policies provided are updated to reflect the variety of assurances expected at sites in the Fedwerated cloud task force, and expected to be supported by CESGA in the future.

.it is clear that using this model and the provided tooos of vmcatcher and vmcaster, the EGI Fed Cloud taskforce needs to develop policies applicable to a wider range of comunites to build this federation.

From our experinace using vmcatcher and vmcaster to implement a federated Applicance management system, we must recomend that the Federated Cloud taskforce provides further work developing apropriate policies to allow endorsers and sites tro agree on a variety of update policies. 

\section{Conclusions and Future work}
\label{sect-conclusions}
The greatest potential for the EGI-federatred cloud task force is to simplify the management of IaaS systems over the native API's.
However Cloud implementation neutrality increases the interoperability challenges. The use of a simple signed in the HVWG proposals makes thier proposals equally applicable to all PKI infrastructures supported by SMIME. The implentation called vmcatcher is not so flexabler but the limitastions did not not hindered this investigation.

Having tested the use of Hepix proposed image distribution system, and the associated tools vmcatcher and vmcaster and the event handlers developed my the Federated Cloud Task force have proved themself as a distributed system for image distribution, 

Taking these requirements into account Fedcloud task force have chosen VMcaster and VMcatcher utilities as production grade implentations of the HVWG's recomendations, will conmtinue to be uses them to distribute and validate VM images between the resource providers, but the policy presented by the Fedcloud task force will need to update the work of the HVWG in the light of developments in clouds and experince testign the policies.

We believe that the HVWG proposal to manage Apliance lifecycle, through signed messages is expresive enough, for its intended perpose. Appliance management is typically IO bound and so asynconous in nature signed images list provide a simple and clear way of managing appliance images are cheched by reource providers. However the EGI Fed Cloud taskforce needs to develop policies applicable to a wider range of sites as plans at CESGA amungst other sites intend to apply greater network isolation between user comunites.
\section*{Acknowledgements}
\label{sect-acknowledgements}
This work is partially funded by the  EGI-InSPIRE (European Grid Initiative: Integrated Sustainable
Pan-European Infrastructure for Researchers in Europe) is a project co-funded by the European Commission 
(contract number INFSO-RI-261323) as an Integrated Infrastructure Initiative within the 7th Framework 
Programme. EGI-InSPIRE began in May 2010 and will run for 4 years. Full information is available at:
\url{http://www.egi.eu/}.

%
% ---- Bibliography ----
%
%\begin{thebibliography}{99}
%
\bibliographystyle{abbrv}

%\bibliographystyle{thebibliography}
\bibliography{bibliography}

%\bibitem{dcache}
%\newblock The dcache book. 
%\newblock \url{http://www.dcache.org/manuals/Book/}, May 2013.


%\bibitem{Django2013}
%\newblock D. Armstrong et al.
%\newblock Runtime virtual machine recontextualization for clouds.
%\newblock Euro-Par 2012: Parallel Processing Workshops. Volume 7640 of Lecture Notes in Computer Science, pages 567--576. Springer Berlin Heidelberg, 2013.

%\bibitem{hepix}
%T.~Cass.
%\newblock The hepix virtualisation working group: Towards a grid of clouds.
%\newblock {\em Journal of Physics: Conference Series}, 396(3):032020, 2012.

%\bibitem{Diaz2012}
%J.~Diaz, G.~von Laszewski, F.~Wang, and G.~Fox.
%\newblock Abstract image management and universal image registration for
%  {C}loud and {HPC} infrastructures.
%\newblock In {\em Cloud Computing (CLOUD), 2012 IEEE 5th International
%  Conference on}, pages 463--470, 2012.

%\bibitem{Dillon2010}
%T.~Dillon, C.~Wu, and E.~Chang.
%\newblock Cloud computing: Issues and challenges.
%\newblock In {\em Advanced Information Networking and Applications (AINA), 2010
%  24th IEEE International Conference on}, pages 27--33, 2010.

%\bibitem{Lagar-Cavilla2009}
%L.-C. et~al.
%\newblock Snowflock: rapid virtual machine cloning for cloud computing.
%\newblock In {\em Proceedings of the 4th ACM European conference on Computer
%  systems}, EuroSys '09, pages 1--12, New York, NY, USA, 2009. ACM.

%\bibitem{Goasguen2012}
%S.~Goasguen, B.~Moreira, E.~Roche, and U.~Schwickerath.
%\newblock Lxcloud : a prototype for an internal cloud in hep. experiences and
%  lessons learned.
%\newblock {\em Journal of Physics: Conference Series}, 396(3):032098, 2012.

%\bibitem{Hoffa2008}
%C.~Hoffa, G.~Mehta, T.~Freeman, E.~Deelman, K.~Keahey, B.~Berriman, and
%  J.~Good.
%\newblock On the use of cloud computing for scientific workflows.
%\newblock In {\em eScience, 2008. eScience '08. IEEE Fourth International
% Conference on}, pages 640--645, 2008.

%\bibitem{Xi2012}
%C.~Li, A.~Raghunathan, and N.~Jha.
%\newblock A trusted virtual machine in an untrusted management environment.
%\newblock {\em Services Computing, IEEE Transactions on}, 5(4):472--483, 2012.

%\bibitem{Maurer2013}
%M.~Maurer, I.~Brandic, and R.~Sakellariou.
%\newblock Adaptive resource configuration for cloud infrastructure management.
%\newblock {\em Future Generation Computer Systems}, 29(2):472 -- 487, 2013.

%\bibitem{glancepush}
%M.~Puel.
%\newblock Openstack glancepush service.
%  \url{https://github.com/EGI-FCTF/glancepush/wiki}, 2013.

%\bibitem{onevent}
%R.~Rosende.
%\newblock Opennebula event handler.
%  \url{https://github.com/grid-admin/vmcatcher\_eventHndlExpl\_ON}, May 2013.

%\bibitem{wnodes}
%D.~Salomoni, A.~Italiano, and E.~Ronchieri.
%\newblock Wnodes, a tool for integrated grid and cloud access and computing
%  farm virtualization.
%\newblock {\em Journal of Physics: Conference Series}, 331:052017, 2011.

%\bibitem{Schwarzkopf2012}
%R.~Schwarzkopf, M.~Schmidt, C.~Strack, S.~Martin, and B.~Freisleben.
%\newblock Increasing virtual machine security in cloud environments.
%\newblock {\em Journal of Cloud Computing}, 1(1):1--12, 2012.

%\bibitem{vmcaster}
%O.~Synge.
%\newblock Vmcaster vm image publication tool.
%  \url{https://github.com/hepix-virtualisation/vmcaster}, May 2013.

%\bibitem{vmcatcher}
%O.~Synge.
%\newblock Vmcatcher image subscription tool.
%  \url{https://github.com/hepix-virtualisation/vmcatcher}, May 2013.

%\bibitem{Laszewski2012}
%G.~von Laszewski, J.~Diaz, F.~Wang, and G.~Fox.
%\newblock Comparison of multiple cloud frameworks.
%\newblock In {\em Cloud Computing (CLOUD), 2012 IEEE 5th International
%  Conference on}, pages 734--741, 2012.

%\bibitem{Xiaolong2012}
%X.~Wen, G.~Gu, Q.~Li, Y.~Gao, and X.~Zhang.
%\newblock Comparison of open-source cloud management platforms: Openstack and opennebula.
%\newblock In Fuzzy Systems and Knowledge Discovery (FSKD), 2012 9th
%\newblock  International Conference on, pages 2457--2461, 2012.

%\bibitem{Zhao2012}
%Y.~Zhao, Y.~Zhang, W.~Tian, R.~Xue, and C.~Lin.
%\newblock Designing and deploying a scientific computing cloud platform.
%\newblock In {\em Grid Computing (GRID), 2012 ACM/IEEE 13th International
%  Conference on}, pages 104--113, 2012.

%\end{thebibliography}


\end{document}
