% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.2 for LaTeX2e
%
\documentclass{llncs_Ibergrid2013}
%
\usepackage{makeidx}  % allows for indexgeneration
%
%\usepackage[dvips]{graphicx}
%
\usepackage{epsfig}
\usepackage{cite}
\usepackage{url}
\usepackage{microtype}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{eurosym}
\usepackage{url}
\usepackage{multirow}
\usepackage{listings}
\usepackage{natbib}
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
\addtocmark{Federated Appliance Distribution in a Federated Cloud} % additional mark in the TOC
%
\mainmatter              % start of the contributions
%
\title{Federated Appliance Distribution in a Federated Cloud}
%
\titlerunning{VM Image Management}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{A. Sim\'on\inst{1}, E. Freire\inst{1}, R. Rosende\inst{1}, I. D\'iaz\inst{1}, A. Feij\'oo\inst{1}, P. Rey\inst{1}, J. L\'opez-Cacheiro\inst{1}, C. Fern\'andez\inst{1} and O. Synge\inst{2}}
%
%\authorrunning{First Author et al.}   % abbreviated author list (for running head)
%
%%%% modified list of authors for the TOC (add the affiliations)
\tocauthor{First Author (Institution of first author's affiliation),
Second Author (Institution of second author's affiliation)}
%
\institute{Fundaci\'on Centro de Supercomputaci\'on de Galicia, Santiago de Compostela, Spain\\
\email{grid-admin@cesga.es}
\and
\email{owen.synge@jaysnest.de}
}




\maketitle              % typeset the title of the contribution

\begin{abstract}
With the development of Federated cloud infrastructures, Maintainers of virtual appliances need a mechanism to distribute thier work to sites, ideally without external dependancies that may delay critical updates to virtual appliances. This paper summarises the work developed within EGI FedCloud taskforce during the last year to deploy a sustainable Federated Appliance lifecycle Management System. This allows Appliance Maintainers to manage the lifecycle of Appliances on the Federated services they use without the need for centralised resources or services.
\end{abstract}

%
\section{Introduction}
\label{sect-introduction}
%
With virtual appliances on Cloud IaaS resources the complications for integrating multiple components and dependencies of a service can be managed closer to the appliance creator or maintainer without need for the cloud resource provider to be involved. Cloud IaaS has shown great commercial success, so much so that new cloud software stacks are being developed, and many companies and research institutes are deploying these clouds for both public and private use. EGI-inspire brings together many sites, running multiple IaaS Cloud implementations as a Federated cloud. As part of this effort they are making efforts to help abstract away these differences so that Virtual Appliance users and maintainers can remain IaaS implementation and site neutral.

The liberation of scientific computation from IaaS implentation lock in may have many economic and sustainability benifits as side effects for all IaaS computing.

Interoperability of Appliance distribution is one of the first dependencies in the process, of creating a federated cloud. This paper is focused on some of the issues surrounding Virtual Appliance Image Management in a federated cloud environment following the HVWG's proposals.

A federated cloud using a heterogenous cloud frameworks ecosystem (OpenStack, OpenNebula, WNoDES~\cite{wnodes}, etc). may be presented with corresponding APIs, Appliance formats, and different contextualisation mechanism for each cloud iplementation. The tools used to evaluate the HVWG's proposals did not support any specififc cloud infrstructures at the start of this process, so adaptors had to be devloped for each IaaS System supported. So far OpenStack and  OpenNebula adapors have been developed by members of the Federated Cloud Task Force.

%\subsection{User Services}

\section{Motivation}
\label{sect-motivation}
The priamary goal of a EGI-federatred cloud task force is to invetigate the potential of IaaS Clouds as a revolutionary upgrade to Grid Computing's useability. IaaS Clouds have been been deployued at many sites where they are actively used by communties that never succeeded with Grid Computation~\cite[?]. The EGI-federatred cloud task force are a testbed evaluating policies and technology for production use.

Secondary goals include providing a unification of the IaaS resources, from a users experiance and form resource sharing making for a quality of service to be extended beyond the capoabilities of a financially constrained resorce provider.

This is very similar to the goals of Grid computing but making use of IaaS platforms. IaaS platforms have succeeded in the Market, while Grid computing succeeded only for big science with very homogenious workloads. It is the beleif of the author that this is related to IaaS Clouds, having far greater flexability for the Apliance Maintainer then the Grid Equivilent. The relatively new term DevOps is now common, in IT, and this person is often closly acociated with the role of Appliance management.
%\begin{figure}[h!]
%\centering
%\includegraphics[width=60mm,angle=90]{./PSFIGs/user-services.eps}
%\caption{Use case in IBERGRID}\label{figUSER}
%\end{figure}
\section{Related Work}
\label{sect-relatedwork}
The use of Cloud infrastructures in science has been documented, and it is a very promising field, but integrating Clouds with the Grid is a challenge, as related on Dillon et al.~\cite{Dillon2010}. Goasguen et al.~\cite{Goasguen2012} presents the results of an internal production cloud service in CERN and suggestions to expand it to another Grid sites. Zhao et al.~\cite{Zhao2012} presents a infrastructure of a dozen computing sites using OpenNebula as the management solution. It concludes that Clouds are very useful for science, but there are still many performance issues to be resolved. Hoffa et al.~\cite{Hoffa2008} reached similar conclusions regarding cloud vs local deployments.

There are also many works that compare the different solutions for VM Management, like Xiaolong et al.~\cite{Xiaolong2012}, which compares OpenNebula and Openstack, and Laszewski et al.~\cite{Laszewski2012}, which does a more complete survey including Eucalyptus, Nimbus and some other solutions. The existence of numerous trade-offs and fragmented market for these tools motivated us to support cross-grid environments.

On the realm of security, our solution emphasizes the authentication of users and the validation of VMs. There are other works on the area, but some, like Xi et al.~\cite{Xi2012} are concerned more with running trusted VMs on on untrusted environments, which can be seen as the opposite problem, and many others, like Schwarzkopf et al.~\cite{Schwarzkopf2012} are concerned with improving the internal security of VMs maintained by Cloud users instead of infrastructure operators.

There are still other comparable solutions, Lagar-Cavilla et al.~\cite{Lagar-Cavilla2009} use a non-local fork mechanism to spawn many copies of a VM across many sites, but this method would be at odds with current Grid practices. Diaz et al.~\cite{Diaz2012} have a similar system that bridges OpenNebula and OpenStack, but it uses the Amazon EC2 API, which has licensing issues preventing us for using it, and does not address the authorization and validation of VMs. On a more partial resemblance, Maurer et al.~\cite{Maurer2013} also automates some aspects of VM management and updates using an autonomous system, and Django et al.~\cite{Django2013} changes the context of VMs on the fly to do load balancing and improve brokering. This last functionality would be invaluable for Grid operators, which must frequently tend to processes that get stuck due to unrealistic brokering requirements, and would also avoid many downtimes due to reconfiguration.


During the last year the EGI federated cloud task force has recommended VMcatcher as well for installation at all cloud resource providers in their collaboration and the status of the integration with OpenStack and OpenNebula.

\section{Federated Appliance Management}
\label{sect-fedimagemanagement}

This paper focuses on appliance distribution, in a federated cloud following the HVWG's recoemendations, it tries to detail where the available implementation of the subscriber falls short of the HVWG's recoemendations. It is the combined result of the EGI-federatred cloud task force collabourating with a maintainer of a HVWG image list subscriber, and a former memeber of the Hepix Virtualisation Working Groups, it focuses on the use of Appliance mangement through signd "image list" messages, in a similar way to package management in Linux.

The aim of the HVWG was to suggest ways so sites have a way to control and mange Virtual Machine (VM) Image's provided by experiments, and execute the VMs in a trusted environment (similar to the current computing environment provided under Grid computing). These goals may no long be aproprate as Cloud Technology has developed allowing greater isolation at the network level.
\section{Itentity management}
\label{sect-fedimagemanagement}
The HVWG proposal are based upon SMIME and so is esentially neutral of the method of signing and is applicable to pgp or x.509 signatures. This is not true for the vmcatcher implementation, which can only support the model provided with x.509 security. Since the Grid comunity already use the International Grid Trust Federation~\cite{igtf} just suporting x.509 is acceptable for the current stage for the Federated cloud task force. It is recomended that future HVWG image list providers support pgp and x.509 signatures.

\section{Appliance Lifecycle Management using Imagelists.}
\label{sect-appliancelifecycle}

The HVWG proposed a message system to decouple appliance maintainers from cloud implementations, this decoupling could be used to hide the hetrogenious nature of a federated cloud. Apliance thoguh are needed to be authenticated and for them to be deemed valid, tey must be from th elatest signed imagelist, and have a secure checksum that matches that of the metadata.

Imagelists are polled by the subscribers, thier signature checked, image downloaded and the validity as an appliance checked using the secure hash. If the applicance image is valid the applicance is then able to be contextualised and then instantiated (see figure \ref{fig:infrastructure}). 
Using this mechanism a virtual image can be checked for validity, all image lists are signed and it provide a version number and expiration date. If the image list does not satisfy these requirements the image is not instantiated and the request is rejected.

In many senses image list VMcatcher~\cite{vmcatcher} is similar to Debian's \textit{aptitude} or the rpm update management \textit{yum} utilities or a podcast subscriber. VMcaster is a tool for making secure respotories in a similar way to \textit{create-repo}. But 2 major differences exist in the HVWG's proposals, that each image is uniquely identified by a UUID, and that Apliance image no longer present in an imagelist indicates thier expiry of an appliance rather than an error as it woudl with package management.

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{vmcaster_vmcatcher.png}
\caption{VMcaster and VMcatcher infrastructure.}
\label{fig:infrastructure}
\end{figure}

\subsection{Publishing image lists}
Publishing images according to the HVWG's proposals is as simple as generating the meta data and signing it with standards SMIME signature routines. No new software is needed for this task. Due to the amount of meta data recomeneded by the HVWG and the expectation that users may want to add thier own additional metadata, it is expected that users will use tools for generating the nessisarry JSON 

HVWG image lists use a JSON metadata file signed with SMIME and referancing images with secure hases seems an apropriate interface for the Appliance maintainer. However the amount of metadata recomended by the HVWG is berdonsom to enter by hand.
 
VMcaster~\cite{vmcaster} is the 3rd iyteration of increasingly automated image list publication tools. VMcaster~\cite{vmcaster} automates the process of uploading images and imagelists, generating as much meta data as possible from its local database, and the images while they are available, prior to upload.

\subsection{Subscribing to image lists}

VMcatcher~\cite{vmcatcher} ius currently the only HVWG image list subscriber, sadly the only available implementation only support JSON metadata and x509 signatures. Since this is the the meta data format matters little and the x.509 required secuurity infrastrcuture is available these limitations did not not hindered this investigation.

The job of vmcatcher is to allow subscription of virtual maschien images by thier UUID so that a resorce provider can provide cintecxtulised apliance images from the most uptodate appliance Image. resource providers can configure their own imagelists and also select images from an image list, these are then validated in complainace with the Hepix security policies and cached locally. vmcatcher does not support any cloud exploistly so furterh work was needed in this area.

VMcaster can be configured to launch appliacations on image update events for further applications to process, update or expire changes of Apliance images.  These events can be used by event listeners or event handlers to react to specific changes (when an image is set as invalid or a it is available a new image update). 

The federated Cloud task force has developed such integration handlers for managing Aplication Lifecyle management with a single signed message, not all users will not need to manage Appliance updates in thier federations,

\section{Image management event handlers}
\label{sect-handlers}
\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{ONeventhandler.png}
\caption{OpenNebula event handler.}
\label{fig:onevent}
\end{figure}
OpenNebula event handler~\cite{onevent} was developed by the CESGA team and currently is available from the VMcaster repository. 
The \textit{vmcatcher\_eventHndlExpl\_ON} package provides a new python and cron script which detects VMcatcher events. 
This script detects several VMcatcher event types. In this case OpenNebula handler only waits for a new Expire or Available VMcatcher event.
If VMcatcher raises a \textit{AvailablePostfix} event, this is detected by \textit{vmcatcher\_eventHndlExpl\_ON} event handler and reads the new image attributes such UUID, image name, description and image format.

If a new image is downloaded (\textit{AvailablePostfix} event), the OpenNebula event handler gets the image information, generates a new OpenNebula template and includes the new image into the local OpenNebula datastore (see figure \ref{fig:onevent}). 
For security reasons, the new images are not public, they are only available for the oneadmin user. The OpenNebula administrator should verify the new image first (checking its contextualisation script, if the image is executed correctly etc).
After this period of time the image status is changed to be available for external users. 
Moreover if VMcatcher detects an image revocation the OpenNebula event handler search the image UUID from OpenNebula image database and it is set to disable status.
The image is not removed by the event handler, it should be removed by the site administrator from the OpenNebula datastore.

The OpenNebula event handler is not the only one. OpenStack administrators can also use Glancepush~\cite{glancepush} service to keep their local image catalog updated. 
This service was developed at IN2P3 and it works in a similar way than the OpenNebula event handler. 
In this case Glancepush updates the OpenStack Image Service (Glance) if it detects any image change from VMcatcher tool. 
The new package (\textit{glancepush-vmcatcher}) is available from IN2P3 ftp server, and it only requires a working glance service and an OpenStack user account to push images into the catalog.

%\section{Use Case: EGI SA2.3 verification image repository}
%\label{sect-usecase}
%This is my text 


\section{Experiance from testing the HVWG proposals}
\label{sect-experiances}

The HVWG's policys where tested to allow this distributed model, requires appliance maintainers, to allow images to be executed at a set of sites rarther than contact each indervidually. With these tools we tried to match the requirements set by the now completed HEPiX virtualisation working group~\cite{hepix}.

As demonstrated in the Federated Cloud Demo (we need a citation), vmcatcher and vmcaster worked successfully and are relatively east to deploy and configure. Investigation shows they operate without a single point of failure. Once CESGA impleemnted the nessisary handler to integrate vmcatcher with Open Nebular and similar work was provided by IN2P3 for open stax images can be published for both platforms as contextuliastion validated. Fully automated image updates between more than one site in both directions have been shown to work. Combined with the Hepix security policy, the apliance management system in testing has shown itself to be a basis of a full automated, federated image distribution system in practice and currently prodcution grade.

Testing and practice has shown that not all appliance producers regularly upgrade thier images. If applince maintianers do update it is unusual that they do so at the frequancy expected by the Hepix virtualisation working group. By providing greater isolation at the netwrok level, NIKEF and Oxford University in particular, have long suggested that they are capable of running insecure images, without fear of applinaces negatively effecting each other. CESGA is planing on developing policies and practices to also atchive this in the future. 

In light of the flexability allowed by greater nework isolation, the monthly update recomendations from the Hepix Virtualisation Working group now seem overly optomistic. The flexabilities in Cloud netowrking capabilities where not forseen in the Hepix proposals and consiquently the ability to run poterntially outdated Appliances in a secured networking enviroment, where not addressed by the HVWG.

We found the clear defintion of valid and invalid/expired images, and the distributed nature of imagelists a natural fit to a federated cloud, even if the policies proposed by the HVWG seem overly restrictive for teh Federated Cloud. It shoud be noted that the both the imagelists and image meta data can be extended to support extra metadata and pass this meta data through the vmcaster and vmcatcher infrastructure. This could be used to referance a variety of Policies used to federate images between sites shodu such policies be developed.


\section{Continued use beyond the prototype}
\label{sect-continued}

A new use cases is the EGI SA2.3 verification image repository. The EGI SA2 testbed is used to verify and test the new middleware before reaching the production software repository (UMD).
One of the most important new features is the ability to distribute and publish VM images in an automated way. 
These new tools, like EGI MarketPlace\footnote{EGI MarketPlace: \url{http://marketplace.egi.eu}} and VMcatcher\footnote{VMcatcher: \url{https://github.com/hepix-virtualisation/vmcatcher}} can be also used within SA2 verification process to distribute and publish new UMD services after its verification. 
This work is still on going and it will be available in the next months. Using this infrastructure the new EGI certified image it will be available to be used and tested by EGI site administrators after each successful verification.

For the moment after each software verification the images are stored locally into CESGA OpenNebula datastore but thanks to image management tools like VMcaster the new images will be published in an automated way.
This new paradigm will be useful to users that want to check the latest software changes without the need to install a new service from scratch.
Fortunately as we have explained in this paper, VMcatcher can be used by any cloud framework to distribute and update images in a transparent way. 
The new image management tools are being used by EGI Fedcloud providers since last year successfully and probably it will be used in more use cases in the near future.

\section{Conclusions and Future work}
\label{sect-conclusions}
The greatest potential for the EGI-federatred cloud task force is to simplify the management of IaaS systems over the native API's. However Cloud implementation neutrality increases the interoperability challenges. 

The use of a simple signed in the HVWG proposals makes thier proposals equally applicable to all PKI infrastructures supported by SMIME. The implentation called vmcatcher is not so flexabler but the limitastions did not not hindered this investigation.

Having tested the use of Hepix proposed image distribution system, and the provided tools vmcatcher, vmcaster and having developed the event handlers the Federated Cloud Task force have proved this aproach is a distributed system for image distribution, 

Taking these requirements into account Fedcloud task force have chosen VMcaster and VMcatcher utilities as production grade implentations of the HVWG's recomendations, will conmtinue to be uses them to distribute and validate VM images between the resource providers, but the policy presented by the Fedcloud task force will need to update the work of the HVWG in the light of developments in clouds and experince testign the policies.

We believe that the HVWG proposal to manage Apliance lifecycle, through signed messages is expresive enough, for its intended perpose. Appliance management is typically IO bound and so asynconous in nature signed images list provide a simple and clear way of managing appliance images are cheched by reource providers. However the EGI Fed Cloud taskforce needs to develop policies applicable to a wider range of sites as plans at CESGA amungst other sites intend to apply greater network isolation between user comunites. Thus allowing support of potentially outdated and poorly supported Aplianced. This change was not seen by the HVWG and is clearly a priority in enabling the Federated Cloud.

For these reasons we consider that the infrastructure provided by the Hepix Virtualisation working group forfills the needs of a EGI Tederated Cloud tasjkforce but recomend that the policies provided are updated to reflect differing levels of netwrok isolation available in different sites within the EGI federated cloud at sites in the Fedwerated cloud task force. Thus a federation of sites using the HVWG's imagelist format can be developed.

\section*{Acknowledgements}
\label{sect-acknowledgements}
This work is partially funded by the  EGI-InSPIRE (European Grid Initiative: Integrated Sustainable
Pan-European Infrastructure for Researchers in Europe) is a project co-funded by the European Commission 
(contract number INFSO-RI-261323) as an Integrated Infrastructure Initiative within the 7th Framework 
Programme. EGI-InSPIRE began in May 2010 and will run for 4 years. Full information is available at:
\url{http://www.egi.eu/}.

%
% ---- Bibliography ----
%
%\begin{thebibliography}{99}
%
\bibliographystyle{abbrv}

%\bibliographystyle{thebibliography}
\bibliography{bibliography}

%\bibitem{dcache}
%\newblock The dcache book. 
%\newblock \url{http://www.dcache.org/manuals/Book/}, May 2013.


%\bibitem{Django2013}
%\newblock D. Armstrong et al.
%\newblock Runtime virtual machine recontextualization for clouds.
%\newblock Euro-Par 2012: Parallel Processing Workshops. Volume 7640 of Lecture Notes in Computer Science, pages 567--576. Springer Berlin Heidelberg, 2013.

%\bibitem{hepix}
%T.~Cass.
%\newblock The hepix virtualisation working group: Towards a grid of clouds.
%\newblock {\em Journal of Physics: Conference Series}, 396(3):032020, 2012.

%\bibitem{Diaz2012}
%J.~Diaz, G.~von Laszewski, F.~Wang, and G.~Fox.
%\newblock Abstract image management and universal image registration for
%  {C}loud and {HPC} infrastructures.
%\newblock In {\em Cloud Computing (CLOUD), 2012 IEEE 5th International
%  Conference on}, pages 463--470, 2012.

%\bibitem{Dillon2010}
%T.~Dillon, C.~Wu, and E.~Chang.
%\newblock Cloud computing: Issues and challenges.
%\newblock In {\em Advanced Information Networking and Applications (AINA), 2010
%  24th IEEE International Conference on}, pages 27--33, 2010.

%\bibitem{Lagar-Cavilla2009}
%L.-C. et~al.
%\newblock Snowflock: rapid virtual machine cloning for cloud computing.
%\newblock In {\em Proceedings of the 4th ACM European conference on Computer
%  systems}, EuroSys '09, pages 1--12, New York, NY, USA, 2009. ACM.

%\bibitem{Goasguen2012}
%S.~Goasguen, B.~Moreira, E.~Roche, and U.~Schwickerath.
%\newblock Lxcloud : a prototype for an internal cloud in hep. experiences and
%  lessons learned.
%\newblock {\em Journal of Physics: Conference Series}, 396(3):032098, 2012.

%\bibitem{Hoffa2008}
%C.~Hoffa, G.~Mehta, T.~Freeman, E.~Deelman, K.~Keahey, B.~Berriman, and
%  J.~Good.
%\newblock On the use of cloud computing for scientific workflows.
%\newblock In {\em eScience, 2008. eScience '08. IEEE Fourth International
% Conference on}, pages 640--645, 2008.

%\bibitem{Xi2012}
%C.~Li, A.~Raghunathan, and N.~Jha.
%\newblock A trusted virtual machine in an untrusted management environment.
%\newblock {\em Services Computing, IEEE Transactions on}, 5(4):472--483, 2012.

%\bibitem{Maurer2013}
%M.~Maurer, I.~Brandic, and R.~Sakellariou.
%\newblock Adaptive resource configuration for cloud infrastructure management.
%\newblock {\em Future Generation Computer Systems}, 29(2):472 -- 487, 2013.

%\bibitem{glancepush}
%M.~Puel.
%\newblock Openstack glancepush service.
%  \url{https://github.com/EGI-FCTF/glancepush/wiki}, 2013.

%\bibitem{onevent}
%R.~Rosende.
%\newblock Opennebula event handler.
%  \url{https://github.com/grid-admin/vmcatcher\_eventHndlExpl\_ON}, May 2013.

%\bibitem{wnodes}
%D.~Salomoni, A.~Italiano, and E.~Ronchieri.
%\newblock Wnodes, a tool for integrated grid and cloud access and computing
%  farm virtualization.
%\newblock {\em Journal of Physics: Conference Series}, 331:052017, 2011.

%\bibitem{Schwarzkopf2012}
%R.~Schwarzkopf, M.~Schmidt, C.~Strack, S.~Martin, and B.~Freisleben.
%\newblock Increasing virtual machine security in cloud environments.
%\newblock {\em Journal of Cloud Computing}, 1(1):1--12, 2012.

%\bibitem{vmcaster}
%O.~Synge.
%\newblock Vmcaster vm image publication tool.
%  \url{https://github.com/hepix-virtualisation/vmcaster}, May 2013.

%\bibitem{vmcatcher}
%O.~Synge.
%\newblock Vmcatcher image subscription tool.
%  \url{https://github.com/hepix-virtualisation/vmcatcher}, May 2013.

%\bibitem{Laszewski2012}
%G.~von Laszewski, J.~Diaz, F.~Wang, and G.~Fox.
%\newblock Comparison of multiple cloud frameworks.
%\newblock In {\em Cloud Computing (CLOUD), 2012 IEEE 5th International
%  Conference on}, pages 734--741, 2012.

%\bibitem{Xiaolong2012}
%X.~Wen, G.~Gu, Q.~Li, Y.~Gao, and X.~Zhang.
%\newblock Comparison of open-source cloud management platforms: Openstack and opennebula.
%\newblock In Fuzzy Systems and Knowledge Discovery (FSKD), 2012 9th
%\newblock  International Conference on, pages 2457--2461, 2012.

%\bibitem{Zhao2012}
%Y.~Zhao, Y.~Zhang, W.~Tian, R.~Xue, and C.~Lin.
%\newblock Designing and deploying a scientific computing cloud platform.
%\newblock In {\em Grid Computing (GRID), 2012 ACM/IEEE 13th International
%  Conference on}, pages 104--113, 2012.

%\end{thebibliography}


\end{document}
